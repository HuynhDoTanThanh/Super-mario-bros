{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T15:22:06.175579Z","iopub.status.busy":"2021-12-22T15:22:06.174629Z","iopub.status.idle":"2021-12-22T15:22:13.842651Z","shell.execute_reply":"2021-12-22T15:22:13.841769Z","shell.execute_reply.started":"2021-12-22T15:22:06.175529Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gym-super-mario-bros in c:\\users\\19522\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (7.3.2)\n","Requirement already satisfied: nes-py>=8.1.2 in c:\\users\\19522\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from gym-super-mario-bros) (8.1.8)\n","Requirement already satisfied: gym>=0.17.2 in c:\\users\\19522\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nes-py>=8.1.2->gym-super-mario-bros) (0.21.0)\n","Requirement already satisfied: numpy>=1.18.5 in c:\\users\\19522\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nes-py>=8.1.2->gym-super-mario-bros) (1.21.4)\n","Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in c:\\users\\19522\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nes-py>=8.1.2->gym-super-mario-bros) (1.5.11)\n","Requirement already satisfied: tqdm>=4.48.2 in c:\\users\\19522\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nes-py>=8.1.2->gym-super-mario-bros) (4.62.3)\n","Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\19522\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from gym>=0.17.2->nes-py>=8.1.2->gym-super-mario-bros) (2.0.0)\n","Requirement already satisfied: colorama in c:\\users\\19522\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tqdm>=4.48.2->nes-py>=8.1.2->gym-super-mario-bros) (0.4.4)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install gym-super-mario-bros"]},{"cell_type":"markdown","metadata":{},"source":["# Wrapper"]},{"cell_type":"code","execution_count":2,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-12-22T15:22:13.845635Z","iopub.status.busy":"2021-12-22T15:22:13.845315Z","iopub.status.idle":"2021-12-22T15:22:13.876391Z","shell.execute_reply":"2021-12-22T15:22:13.875507Z","shell.execute_reply.started":"2021-12-22T15:22:13.845593Z"},"trusted":true},"outputs":[],"source":["import gym_super_mario_bros\n","from gym.spaces import Box\n","from gym import Wrapper\n","from nes_py.wrappers import JoypadSpace\n","from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY\n","import cv2\n","import numpy as np\n","import subprocess as sp\n","import torch\n","import torch.multiprocessing as mp\n","\n","\n","class Monitor:\n","    def __init__(self, width, height, saved_path):\n","\n","        self.command = [\"ffmpeg\", \"-y\", \"-f\", \"rawvideo\", \"-vcodec\", \"rawvideo\", \"-s\", \"{}X{}\".format(width, height),\n","                        \"-pix_fmt\", \"rgb24\", \"-r\", \"60\", \"-i\", \"-\", \"-an\", \"-vcodec\", \"mpeg4\", saved_path]\n","        try:\n","            self.pipe = sp.Popen(self.command, stdin=sp.PIPE, stderr=sp.PIPE)\n","        except FileNotFoundError:\n","            pass\n","\n","    def record(self, image_array):\n","        self.pipe.stdin.write(image_array.tostring())\n","\n","\n","def process_frame(frame):\n","    if frame is not None:\n","        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n","        frame = cv2.resize(frame, (84, 84))[None, :, :] / 255.\n","        return frame\n","    else:\n","        return np.zeros((1, 84, 84))\n","\n","\n","class CustomReward(Wrapper):\n","    def __init__(self, env=None, world=None, stage=None, monitor=None):\n","        super(CustomReward, self).__init__(env)\n","        self.observation_space = Box(low=0, high=255, shape=(1, 84, 84))\n","        self.curr_score = 0\n","        self.current_x = 40\n","        self.world = world\n","        self.stage = stage\n","        if monitor:\n","            self.monitor = monitor\n","        else:\n","            self.monitor = None\n","\n","    def step(self, action):\n","        state, reward, done, info = self.env.step(action)\n","        if self.monitor:\n","            self.monitor.record(state)\n","        state = process_frame(state)\n","        reward += (info[\"score\"] - self.curr_score) / 40.\n","        self.curr_score = info[\"score\"]\n","        if done:\n","            if info[\"flag_get\"]:\n","                reward += 50\n","            else:\n","                reward -= 50\n","\n","        self.current_x = info[\"x_pos\"]\n","        return state, reward / 10., done, info\n","\n","    def reset(self):\n","        self.curr_score = 0\n","        self.current_x = 40\n","        return process_frame(self.env.reset())\n","\n","\n","class CustomSkipFrame(Wrapper):\n","    def __init__(self, env, skip=4):\n","        super(CustomSkipFrame, self).__init__(env)\n","        self.observation_space = Box(low=0, high=255, shape=(skip, 84, 84))\n","        self.skip = skip\n","        self.states = np.zeros((skip, 84, 84), dtype=np.float32)\n","\n","    def step(self, action):\n","        total_reward = 0\n","        last_states = []\n","        for i in range(self.skip):\n","            state, reward, done, info = self.env.step(action)\n","            total_reward += reward\n","            if i >= self.skip / 2:\n","                last_states.append(state)\n","            if done:\n","                self.reset()\n","                return self.states[None, :, :, :].astype(np.float32), total_reward, done, info\n","        max_state = np.max(np.concatenate(last_states, 0), 0)\n","        self.states[:-1] = self.states[1:]\n","        self.states[-1] = max_state\n","        return self.states[None, :, :, :].astype(np.float32), total_reward, done, info\n","\n","    def reset(self):\n","        state = self.env.reset()\n","        self.states = np.concatenate([state for _ in range(self.skip)], 0)\n","        return self.states[None, :, :, :].astype(np.float32)\n","\n","\n","def create_train_env(world, stage, actions, output_path=None):\n","    env = gym_super_mario_bros.make(\"SuperMarioBros-{}-{}-v0\".format(world, stage))\n","    if output_path:\n","        monitor = Monitor(256, 240, output_path)\n","    else:\n","        monitor = None\n","\n","    env = JoypadSpace(env, actions)\n","    env = CustomReward(env, world, stage, monitor)\n","    env = CustomSkipFrame(env)\n","    return env\n"]},{"cell_type":"markdown","metadata":{},"source":["# Agent"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T15:22:13.878486Z","iopub.status.busy":"2021-12-22T15:22:13.878123Z","iopub.status.idle":"2021-12-22T15:22:13.890976Z","shell.execute_reply":"2021-12-22T15:22:13.890253Z","shell.execute_reply.started":"2021-12-22T15:22:13.878409Z"},"trusted":true},"outputs":[],"source":["class MultipleEnvironments:\n","    def __init__(self, world, stage, action_type, num_envs, output_path=None):\n","        self.agent_conns, self.env_conns = zip(*[mp.Pipe() for _ in range(num_envs)])\n","        if action_type == \"right\":\n","            actions = RIGHT_ONLY\n","        elif action_type == \"simple\":\n","            actions = SIMPLE_MOVEMENT\n","        else:\n","            actions = COMPLEX_MOVEMENT\n","        self.envs = [create_train_env(world, stage, actions, output_path=output_path) for _ in range(num_envs)]\n","        self.num_states = self.envs[0].observation_space.shape[0]\n","        self.num_actions = len(actions)\n","        for index in range(num_envs):\n","            process = mp.Process(target=self.run, args=(index,))\n","            process.start()\n","            self.env_conns[index].close()\n","\n","    def run(self, index):\n","        self.agent_conns[index].close()\n","        while True:\n","            request, action = self.env_conns[index].recv()\n","            if request == \"step\":\n","                self.env_conns[index].send(self.envs[index].step(action.item()))\n","            elif request == \"reset\":\n","                self.env_conns[index].send(self.envs[index].reset())\n","            else:\n","                raise NotImplementedError"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T15:22:13.894524Z","iopub.status.busy":"2021-12-22T15:22:13.894249Z","iopub.status.idle":"2021-12-22T15:22:13.906767Z","shell.execute_reply":"2021-12-22T15:22:13.905964Z","shell.execute_reply.started":"2021-12-22T15:22:13.894495Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","from collections import deque\n","\n","\n","class PPO(nn.Module):\n","    def __init__(self, num_inputs, num_actions):\n","        super(PPO, self).__init__()\n","        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n","        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n","        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n","        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n","        self.linear = nn.Linear(32 * 6 * 6, 512)\n","        self.critic_linear = nn.Linear(512, 1)\n","        self.actor_linear = nn.Linear(512, num_actions)\n","        self._initialize_weights()\n","\n","    def _initialize_weights(self):\n","        for module in self.modules():\n","            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n","                nn.init.orthogonal_(module.weight, nn.init.calculate_gain('relu'))\n","                # nn.init.xavier_uniform_(module.weight)\n","                # nn.init.kaiming_uniform_(module.weight)\n","                nn.init.constant_(module.bias, 0)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = F.relu(self.conv4(x))\n","        x = self.linear(x.view(x.size(0), -1))\n","        return self.actor_linear(x), self.critic_linear(x)"]},{"cell_type":"markdown","metadata":{},"source":["# Eval"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def eval(opt, global_model, num_states, num_actions):\n","    torch.manual_seed(123)\n","    if opt.action_type == \"right\":\n","        actions = RIGHT_ONLY\n","    elif opt.action_type == \"simple\":\n","        actions = SIMPLE_MOVEMENT\n","    else:\n","        actions = COMPLEX_MOVEMENT\n","    env = create_train_env(opt.world, opt.stage, actions)\n","    local_model = PPO(num_states, num_actions)\n","    if torch.cuda.is_available():\n","        local_model.cuda()\n","    local_model.eval()\n","    state = torch.from_numpy(env.reset())\n","    if torch.cuda.is_available():\n","        state = state.cuda()\n","    done = True\n","    curr_step = 0\n","    actions = deque(maxlen=opt.max_actions)\n","    while True:\n","        curr_step += 1\n","        if done:\n","            local_model.load_state_dict(global_model.state_dict())\n","        logits, value = local_model(state)\n","        policy = F.softmax(logits, dim=1)\n","        action = torch.argmax(policy).item()\n","        state, reward, done, info = env.step(action)\n","        if info[\"flag_get\"]:\n","            print(\"Finished\")\n","            torch.save(local_model.state_dict(),\n","                \"{}/ppo_super_mario_bros_{}_{}_{}\".format(opt.saved_path, opt.world, opt.stage, curr_step))\n","\n","        actions.append(action)\n","        if curr_step > opt.num_global_steps or actions.count(actions[0]) == actions.maxlen:\n","            done = True\n","        if done:\n","            curr_step = 0\n","            actions.clear()\n","            state = env.reset()\n","        state = torch.from_numpy(state)\n","        if torch.cuda.is_available():\n","            state = state.cuda()"]},{"cell_type":"markdown","metadata":{},"source":["# Test\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T15:22:13.909196Z","iopub.status.busy":"2021-12-22T15:22:13.908207Z","iopub.status.idle":"2021-12-22T15:22:13.920581Z","shell.execute_reply":"2021-12-22T15:22:13.919911Z","shell.execute_reply.started":"2021-12-22T15:22:13.909153Z"},"trusted":true},"outputs":[],"source":["def test(opt, model):\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(123)\n","    else:\n","        torch.manual_seed(123)\n","    if opt.action_type == \"right\":\n","        actions = RIGHT_ONLY\n","    elif opt.action_type == \"simple\":\n","        actions = SIMPLE_MOVEMENT\n","    else:\n","        actions = COMPLEX_MOVEMENT\n","    env = create_train_env(opt.world, opt.stage, actions)\n","    model.eval()\n","    state = torch.from_numpy(env.reset())\n","    total_reward = 0\n","    while True:\n","        if torch.cuda.is_available():\n","            state = state.cuda()\n","        logits, value = model(state)\n","        policy = F.softmax(logits, dim=1)\n","        action = torch.argmax(policy).item()\n","        state, reward, done, info = env.step(action)\n","        state = torch.from_numpy(state)\n","        total_reward += reward\n","        if done:\n","            if info[\"flag_get\"]:\n","                print(\"---------World {} stage {} completed-------------\".format(opt.world, opt.stage))\n","                return True\n","            print(\"---------Total reward: {}------------------------\".format(total_reward))\n","            return False"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T15:22:13.923471Z","iopub.status.busy":"2021-12-22T15:22:13.922972Z","iopub.status.idle":"2021-12-22T15:22:13.954087Z","shell.execute_reply":"2021-12-22T15:22:13.953326Z","shell.execute_reply.started":"2021-12-22T15:22:13.923408Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","os.environ['OMP_NUM_THREADS'] = '1'\n","import argparse\n","import torch\n","import torch.multiprocessing as _mp\n","from torch.distributions import Categorical\n","import torch.nn.functional as F\n","import numpy as np\n","import shutil\n","\n","def train(opt):\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(123)\n","    else:\n","        torch.manual_seed(123)\n","    if os.path.isdir(opt.log_path):\n","        shutil.rmtree(opt.log_path)\n","    os.makedirs(opt.log_path)\n","    if not os.path.isdir(opt.saved_path):\n","        os.makedirs(opt.saved_path)\n","    mp = _mp.get_context(\"spawn\")\n","    envs = MultipleEnvironments(opt.world, opt.stage, opt.action_type, opt.num_processes)\n","    model = PPO(envs.num_states, envs.num_actions)\n","    if torch.cuda.is_available():\n","        model.cuda()\n","    model.share_memory()\n","    process = mp.Process(target=eval, args=(opt, model, envs.num_states, envs.num_actions))\n","    process.start()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n","    [agent_conn.send((\"reset\", None)) for agent_conn in envs.agent_conns]\n","    curr_states = [agent_conn.recv() for agent_conn in envs.agent_conns]\n","    curr_states = torch.from_numpy(np.concatenate(curr_states, 0))\n","    if torch.cuda.is_available():\n","        curr_states = curr_states.cuda()\n","    curr_episode = 0\n","    while True:\n","        if curr_episode % opt.save_interval == 0 and curr_episode > 0:\n","             torch.save(model.state_dict(),\n","                        \"{}/ppo_super_mario_bros_{}_{}\".format(opt.saved_path, opt.world, opt.stage))\n","             torch.save(model.state_dict(),\n","                        \"{}/ppo_super_mario_bros_{}_{}_{}\".format(opt.saved_path, opt.world, opt.stage, curr_episode))\n","             if test(opt, model):\n","                    break\n","             \n","        curr_episode += 1\n","        old_log_policies = []\n","        actions = []\n","        values = []\n","        states = []\n","        rewards = []\n","        dones = []\n","        for _ in range(opt.num_local_steps):\n","            states.append(curr_states)\n","            logits, value = model(curr_states)\n","            values.append(value.squeeze())\n","            policy = F.softmax(logits, dim=1)\n","            old_m = Categorical(policy)\n","            action = old_m.sample()\n","            actions.append(action)\n","            old_log_policy = old_m.log_prob(action)\n","            old_log_policies.append(old_log_policy)\n","            if torch.cuda.is_available():\n","                [agent_conn.send((\"step\", act)) for agent_conn, act in zip(envs.agent_conns, action.cpu())]\n","            else:\n","                [agent_conn.send((\"step\", act)) for agent_conn, act in zip(envs.agent_conns, action)]\n","\n","            state, reward, done, info = zip(*[agent_conn.recv() for agent_conn in envs.agent_conns])\n","            state = torch.from_numpy(np.concatenate(state, 0))\n","            if torch.cuda.is_available():\n","                state = state.cuda()\n","                reward = torch.cuda.FloatTensor(reward)\n","                done = torch.cuda.FloatTensor(done)\n","            else:\n","                reward = torch.FloatTensor(reward)\n","                done = torch.FloatTensor(done)\n","            rewards.append(reward)\n","            dones.append(done)\n","            curr_states = state\n","\n","        _, next_value, = model(curr_states)\n","        next_value = next_value.squeeze()\n","        old_log_policies = torch.cat(old_log_policies).detach()\n","        actions = torch.cat(actions)\n","        values = torch.cat(values).detach()\n","        states = torch.cat(states)\n","        gae = 0\n","        R = []\n","        for value, reward, done in list(zip(values, rewards, dones))[::-1]:\n","            gae = gae * opt.gamma * opt.tau\n","            gae = gae + reward + opt.gamma * next_value.detach() * (1 - done) - value.detach()\n","            next_value = value\n","            R.append(gae + value)\n","        R = R[::-1]\n","        R = torch.cat(R).detach()\n","        advantages = R - values\n","        for i in range(opt.num_epochs):\n","            indice = torch.randperm(opt.num_local_steps * opt.num_processes)\n","            for j in range(opt.batch_size):\n","                batch_indices = indice[\n","                                int(j * (opt.num_local_steps * opt.num_processes / opt.batch_size)): int((j + 1) * (\n","                                        opt.num_local_steps * opt.num_processes / opt.batch_size))]\n","                logits, value = model(states[batch_indices])\n","                new_policy = F.softmax(logits, dim=1)\n","                new_m = Categorical(new_policy)\n","                new_log_policy = new_m.log_prob(actions[batch_indices])\n","                ratio = torch.exp(new_log_policy - old_log_policies[batch_indices])\n","                actor_loss = -torch.mean(torch.min(ratio * advantages[batch_indices],\n","                                                   torch.clamp(ratio, 1.0 - opt.epsilon, 1.0 + opt.epsilon) *\n","                                                   advantages[\n","                                                       batch_indices]))\n","                critic_loss = torch.mean((R[batch_indices] - value) ** 2) / 2\n","                entropy_loss = torch.mean(new_m.entropy())\n","                total_loss = actor_loss + critic_loss - opt.beta * entropy_loss\n","                optimizer.zero_grad()\n","                total_loss.backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","                optimizer.step()\n","        print(\"Episode: {}. Total loss: {}.\".format(curr_episode, total_loss))\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T15:22:13.955703Z","iopub.status.busy":"2021-12-22T15:22:13.9553Z","iopub.status.idle":"2021-12-22T15:22:13.965993Z","shell.execute_reply":"2021-12-22T15:22:13.965346Z","shell.execute_reply.started":"2021-12-22T15:22:13.955664Z"},"trusted":true},"outputs":[],"source":["class Param:\n","    def __init__(self, world = 1, stage = 1, action_type = \"simple\"):\n","        self.lr = 1e-4\n","        self.gamma = 0.9\n","        self.tau = 1.0\n","        self.beta = 0.01\n","        self.epsilon = 0.2\n","        self.batch_size = 16\n","        self.num_epochs = 10\n","        self.num_local_steps = 512\n","        self.num_global_steps = 5e6\n","        self.num_processes = 8\n","        self.save_interval = 50\n","        self.max_actions = 200\n","        self.log_path = \"tensorboard/ppo_super_mario_bros\"\n","        self.saved_path = \"trained_models\"\n","        self.world = world\n","        self.stage = stage\n","        self.action_type = action_type"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T15:22:13.967682Z","iopub.status.busy":"2021-12-22T15:22:13.967397Z","iopub.status.idle":"2021-12-22T15:22:13.975264Z","shell.execute_reply":"2021-12-22T15:22:13.974514Z","shell.execute_reply.started":"2021-12-22T15:22:13.967647Z"},"trusted":true},"outputs":[],"source":["world = 2\n","stage = 1\n","action_type = \"simple\" \n","opt = Param(world, stage, action_type)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T15:22:13.978632Z","iopub.status.busy":"2021-12-22T15:22:13.977333Z","iopub.status.idle":"2021-12-22T15:40:12.382598Z","shell.execute_reply":"2021-12-22T15:40:12.381396Z","shell.execute_reply.started":"2021-12-22T15:22:13.978593Z"},"trusted":true},"outputs":[],"source":["train(opt)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"}},"nbformat":4,"nbformat_minor":4}
